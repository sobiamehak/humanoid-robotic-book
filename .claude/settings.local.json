{
  "permissions": {
    "allow": [
      "Bash(mkdir:*)",
      "Bash(npm install)",
      "Bash(curl -X GET http://localhost:8000/api/health)",
      "Bash(curl -X GET http://localhost:6333/dashboard)",
      "Bash(python -m uvicorn src.main:app --host 0.0.0.0 --port 8000)",
      "Bash(curl -X POST http://localhost:8000/api/chat -H \"Content-Type: application/json\" -d \"{\"\"message\"\":\"\"Hello\"\", \"\"selected_text\"\":null, \"\"current_page\"\":null}\")",
      "Bash(docker --version)",
      "Bash(python scripts/index_content.py)",
      "Bash(python -c \"import sys; sys.path.insert\\(0, ''.''\\); from scripts.index_content import index_content; index_content\\(\\)\")",
      "Bash(curl -X POST http://localhost:8000/api/chat -H \"Content-Type: application/json\" -d \"{\"\"message\"\":\"\"What is humanoid robotics?\"\", \"\"selected_text\"\":null, \"\"current_page\"\":null}\")",
      "Bash(python -c \"\nimport sys\nsys.path.insert\\(0, ''.''\\)\nfrom src.services.rag import retrieve\ntry:\n    results = retrieve\\(''humanoid robotics'', top_k=3\\)\n    print\\(''Qdrant connection successful''\\)\n    print\\(f''Found {len\\(results\\)} results''\\)\n    for i, result in enumerate\\(results[:2]\\):  # Show first 2 results\n        text_preview = result[''chunk''][''text''][:100] if ''chunk'' in result and ''text'' in result[''chunk''] else ''No text''\n        print\\(f''Result {i+1}: {text_preview}...''\\)\nexcept Exception as e:\n    print\\(f''Error connecting to Qdrant: {e}''\\)\n    import traceback\n    traceback.print_exc\\(\\)\n\")",
      "Bash(python -c \"\nimport sys\nsys.path.insert\\(0, ''.''\\)\nfrom src.services.gemini_service import GeminiService\nimport asyncio\n\nasync def test\\(\\):\n    service = GeminiService\\(\\)\n    print\\(f''Gemini service initialized: {service.is_connected}''\\)\n    if service.is_connected:\n        try:\n            response = await service.generate_response\\(''Hello'', ''Test context''\\)\n            print\\(f''Response: {response}''\\)\n        except Exception as e:\n            print\\(f''Error during generation: {e}''\\)\n            import traceback\n            traceback.print_exc\\(\\)\n    else:\n        print\\(''Gemini service not connected''\\)\n\nasyncio.run\\(test\\(\\)\\)\n\")",
      "Bash(python -c \"\nimport sys\nsys.path.insert\\(0, ''.''\\)\nfrom src.services.local_response_service import generate_response\n\n# Test if the local response service works properly\ntest_context = ''''''# Chapter 12 - Future of Humanoid Robotics\n\nHumanoid robotics is a branch of robotics that focuses on creating robots with human-like characteristics and capabilities. These robots are designed to interact with humans and operate in human environments. The field combines mechanical engineering, artificial intelligence, and human-computer interaction to create machines that can perform tasks traditionally done by humans.\n\n# Chapter 5 - Perception Systems in Humanoid Robots\n\nPerception systems in humanoid robots are critical for understanding and interacting with the environment. These systems typically include cameras, microphones, touch sensors, and other sensory devices that allow the robot to perceive its surroundings.\n\n## Key Components\n\nHumanoid robots typically include:\n- Human-like body structure with head, torso, arms, and legs\n- Advanced sensors for perception\n- Sophisticated control systems for movement\n- AI systems for decision making and interaction\n\n## Applications\n\nHumanoid robots are used in various fields including healthcare, customer service, education, and research.''''''\n\nresponse = generate_response\\(''What is humanoid robotics?'', test_context\\)\nprint\\(''Local response:''\\)\nprint\\(response\\)\n\")",
      "Bash(python -c \"\nimport sys\nsys.path.insert\\(0, ''.''\\)\nfrom src.routes.chat import chat_stream\nimport asyncio\n\nasync def test_stream\\(\\):\n    print\\(''Testing chat stream function...''\\)\n    stream = chat_stream\\(''What is humanoid robotics?''\\)\n    \n    chunks = []\n    async for chunk in stream:\n        chunks.append\\(chunk\\)\n        if chunk.get\\(''event''\\) == ''done'':\n            break\n    \n    print\\(''Received chunks:''\\)\n    for chunk in chunks:\n        print\\(f''  {chunk}''\\)\n\n# This might fail if the Qdrant connection is required during initialization\ntry:\n    asyncio.run\\(test_stream\\(\\)\\)\nexcept Exception as e:\n    print\\(f''Error testing stream: {e}''\\)\n    import traceback\n    traceback.print_exc\\(\\)\n\")",
      "Bash(python -m uvicorn src.main:app --host 0.0.0.0 --port 8000 --reload)",
      "Bash(taskkill /PID 3920 /F)",
      "Bash(python -m uvicorn src.main:app --host 127.0.0.1 --port 8001 --reload)",
      "Bash(curl -X POST http://127.0.0.1:8001/api/chat -H \"Content-Type: application/json\" -d \"{\"\"message\"\":\"\"hi\"\", \"\"selected_text\"\":null, \"\"current_page\"\":null}\")",
      "Bash(curl http://127.0.0.1:8001/api/health)",
      "Bash(python -c \"\nimport sys\nimport asyncio\nsys.path.insert\\(0, ''.''\\)\n\nfrom src.services.llm_service import LLMService\n\nasync def test\\(\\):\n    service = LLMService\\(\\)\n    print\\(f''LLM service initialized: {service.primary_service is not None}''\\)\n    print\\(f''Using service type: {type\\(service.primary_service\\).__name__ if service.primary_service else \"\"None\"\"}''\\)\n    \n    if service.primary_service:\n        try:\n            response = await service.generate_response\\(''Hello'', ''Test context''\\)\n            print\\(f''Response: {response}''\\)\n        except Exception as e:\n            print\\(f''Error during generation: {e}''\\)\n            import traceback\n            traceback.print_exc\\(\\)\n    else:\n        print\\(''No LLM service available''\\)\n\nasyncio.run\\(test\\(\\)\\)\n\")",
      "Bash(taskkill /F /PID 13692)",
      "Bash(netstat -ano)",
      "Bash(findstr :8001)",
      "Bash(taskkill:*)",
      "Bash(taskkill /F /PID:*)",
      "Bash(powershell -Command \"Stop-Process -Id 12120,16784 -Force -ErrorAction SilentlyContinue\":*)",
      "Bash(python -m uvicorn src.main:app --host 127.0.0.1 --port 8080 --reload)",
      "Bash(curl -X POST http://127.0.0.1:8080/api/chat -H \"Content-Type: application/json\" -d \"{\"\"message\"\":\"\"hello\"\", \"\"selected_text\"\":null, \"\"current_page\"\":null}\")",
      "Bash(curl http://127.0.0.1:8080/api/health)",
      "Bash(curl -X POST http://127.0.0.1:8080/api/chat -H \"Content-Type: application/json\" -d \"{\"\"message\"\":\"\"hi\"\", \"\"selected_text\"\":null, \"\"current_page\"\":null}\")",
      "Bash(curl -X POST http://127.0.0.1:8080/api/chat -H \"Content-Type: application/json\" -d \"{\"\"message\"\":\"\"What is the weather today?\"\", \"\"selected_text\"\":null, \"\"current_page\"\":null}\")",
      "Bash(curl -X POST http://127.0.0.1:8080/api/chat -H \"Content-Type: application/json\" -d \"{\"\"message\"\":\"\"What is your favorite color?\"\", \"\"selected_text\"\":null, \"\"current_page\"\":null}\")",
      "Bash(curl -X POST http://127.0.0.1:8080/api/chat -H \"Content-Type: application/json\" -d \"{\"\"message\"\":\"\"How to cook pasta?\"\", \"\"selected_text\"\":null, \"\"current_page\"\":null}\")",
      "Bash(curl -X POST http://127.0.0.1:8080/api/chat -H \"Content-Type: application/json\" -d \"{\"\"message\"\":\"\"What are humanoid robots?\"\", \"\"selected_text\"\":null, \"\"current_page\"\":null}\")",
      "Bash(curl -X POST http://127.0.0.1:8080/api/chat -H \"Content-Type: application/json\" -d \"{\"\"message\"\":\"\"What is your favorite movie?\"\", \"\"selected_text\"\":null, \"\"current_page\"\":null}\")",
      "Bash(curl -X POST http://127.0.0.1:8080/api/chat -H \"Content-Type: application/json\" -d \"{\"\"message\"\":\"\"Tell me about football\"\", \"\"selected_text\"\":null, \"\"current_page\"\":null}\")",
      "Bash(curl -X POST http://127.0.0.1:8080/api/chat -H \"Content-Type: application/json\" -d \"{\"\"message\"\":\"\"What is perception in robotics?\"\", \"\"selected_text\"\":null, \"\"current_page\"\":null}\")"
    ],
    "deny": [],
    "ask": []
  }
}
